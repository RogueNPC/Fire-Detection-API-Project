{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ _DeepFire_: API Project for Fire Detection ðŸ”¥\n",
    "\n",
    "In this project, you'll apply your skills at neural network development in a new way: taking a model that you've trained yourself and deploying it to a static webpage that you can work with to upload new images and get prediction accuracy results. \n",
    "\n",
    "This project will primarily focus on your abilities in creating and testing neural network architecture development. \n",
    "\n",
    "#### **Specifically, you'll be creating a convolutional neural network that can ingest Fire Detection Image Data and predict binary class values, similarly to what we've done with multilayer perceptrons in the past.**\n",
    "\n",
    "Boilerplate and supporting architectures have been provided for a multitude of tasks ranging from data preprocessing, processing, ingestion, and predictive assessment â€“Â however, major tasks and design work will ultimately be left to you to approach and figure out ideal, optimized solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ General Importations\n",
    "\n",
    "As always, we'll start with importing basic tools and functions for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0-dev20210922\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import utils\n",
    "\n",
    "import os, PIL\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Initializing Deep Learning Tools ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Your first task will be crucial to ensuring the successful implementation of the rest of your notebook. \n",
    "> \n",
    "> **Initialize each line with the correct function type from the TensorFlow documentation.**\n",
    "> \n",
    "> Feel free to refer throughout the notebook and across previous notebooks to see which TensorFlow architectures you've used for similar tasks. \n",
    "> \n",
    "> To give you a guide for how this should look, you've been provided with a single correct function declaration in the form of `image_dataset_from_directory` at the end of the cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sequential Model Architecture \"\"\"\n",
    "# TODO: Initialize the sequential model architecture here.\n",
    "Sequential = tf.keras.models.Sequential\n",
    "\n",
    "\"\"\" Data Preprocessing Functions \"\"\"\n",
    "# TODO: Initialize the experimental resizing layer here.\n",
    "Resizing = tf.keras.layers.Resizing\n",
    "# TODO: Initialize the experimental rescaling layer here.\n",
    "Rescaling = tf.keras.layers.Rescaling\n",
    "\n",
    "\"\"\" Data Augmentation Functions \"\"\"\n",
    "# TODO: Initialize the experimental random flipping layer here.\n",
    "RandomFlip = tf.keras.layers.RandomFlip\n",
    "# TODO: Initialize the experimental random rotating layer here.\n",
    "RandomRotation = tf.keras.layers.RandomRotation\n",
    "# TODO: Initialize the experimental random zooming layer here.\n",
    "RandomZoom = tf.keras.layers.RandomZoom\n",
    "\n",
    "\"\"\" Artificial Neural Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the dense connective layer here.\n",
    "Dense = tf.keras.layers.Dense\n",
    "# TODO: Initialize the dropout regularization layer here.\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "\"\"\" Convolutional Neural Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the 2D convolutional layer here.\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "# TODO: Initialize the 2D max pooling layer here.\n",
    "MaxPool2D = tf.keras.layers.MaxPooling2D\n",
    "# TODO: Initialize the flattening layer here.\n",
    "Flatten = tf.keras.layers.Flatten\n",
    "\n",
    "\"\"\" Residual Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the Residual Network multilayer model here.\n",
    "# TODO: Make sure you initialize the 50-layer residual network! \n",
    "# HINT: Look up `ResNet50` for appropriate documentation. \n",
    "ResNet50 = tf.keras.applications.resnet50.ResNet50\n",
    "\n",
    "\"\"\" Function to Load Images from Target Folder \"\"\"\n",
    "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Precheck Image Dataset Sizes\n",
    "\n",
    "If you've followed instructions carefully from the `project/PROJECT.md` instructions, the following dataset directory instantiations should work perfectly. \n",
    "\n",
    "If they do not, double-check to make sure you've saved your dataset to the appropriate location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fire image samples: 110\n",
      "Number of non-fire image samples: 541\n"
     ]
    }
   ],
   "source": [
    "# Use the `glob.glob` function to show how many images are in each folder\n",
    "DATA_DIRECTORY = \"../dataset/Images/\"\n",
    "FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Fire_Images/*\"\n",
    "NOT_FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Normal_Images/*\"\n",
    "\n",
    "print(f\"Number of fire image samples: {len(glob(FIRE_IMAGES_PATTERN))}\")\n",
    "print(f\"Number of non-fire image samples: {len(glob(NOT_FIRE_IMAGES_PATTERN))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¸ Load Dataset\n",
    "\n",
    "Like we've done previously, let's set our batch size and image dimensions to work seamlessly with our configured model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 files belonging to 2 classes.\n",
      "Using 866 files for training.\n"
     ]
    }
   ],
   "source": [
    "# needed to remove error caused by tf-nightly & image_dataset_from_directory\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "train = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 files belonging to 2 classes.\n",
      "Using 216 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can actually see that we have a major class imbalance with our fire images representing our minority class. \n",
    "\n",
    "Let's go ahead and fix that by resampling our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Resample (Oversample) Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_class_distribution(train, DATA_DIRECTORY=DATA_DIRECTORY, save=True):\n",
    "    \"\"\" Helper function to resample class distribution for image dataset. \"\"\"\n",
    "    minority_class, majority_class = list(), list()\n",
    "    for images, labels, in train.take(3):\n",
    "        for image, label in zip(images, labels):\n",
    "            if label == 0:\n",
    "                minority_class.append(image.numpy().astype(np.uint8))\n",
    "            else:\n",
    "                majority_class.append(image.numpy().astype(np.uint8))\n",
    "    FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Fire_Images/*\"))\n",
    "    NOT_FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Normal_Images/*\"))\n",
    "    upsampled_images = np.array(utils.resample(minority_class, replace=True, \n",
    "                                               n_samples=(NOT_FIRE_SIZE - FIRE_SIZE),\n",
    "                                               random_state=42))\n",
    "    if save == True:\n",
    "        index = 0\n",
    "        for image in upsampled_images:\n",
    "            PATH = f\"{DATA_DIRECTORY}/Fire_Images/new_fire_{index}.png\"\n",
    "            PIL.Image.fromarray(image).save(PATH)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_class_distribution(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see that additional images have been generated to balance out both classes prior to predictive modeling.\n",
    "\n",
    "**Go ahead and re-run the `Load Dataset` steps to see new generated dataset changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Pre-Optimize Image File Ingestion\n",
    "\n",
    "This is an accessory step to optimize image data ingestion at the cost of slightly higher memory usage. No modifications are required for this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_performant_datasets(dataset, shuffling=None):\n",
    "    \"\"\" \n",
    "    Custom function to prefetch and cache stored elements\n",
    "    of retrieved image data to boost latency and performance\n",
    "    at the cost of higher memory usage. \n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    # Cache and prefetch elements of input data for boosted performance\n",
    "    if not shuffling:\n",
    "        return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        return dataset.cache().shuffle(shuffling).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =         configure_performant_datasets(train, shuffling=1000)\n",
    "validation =    configure_performant_datasets(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Create Resizing and Normalization Layers ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll declare your resizing and normalization layers using the layer architectures that you imported earlier. \n",
    ">\n",
    "> Recall that for this step, we want to accomplish two key tasks: \n",
    "> - Resize all images to the predetermined square image dimensions as indicated by `IMAGE_HEIGHT` and `IMAGE_WIDTH`.\n",
    "> - Scale all images so pixel values are within the range of (0., 1.) rather than the original (0., 255.).\n",
    ">\n",
    "> Additionally, since we're working with colorized image data, we'll want to ensure that our image rescaling/normalization step inputs images as stacks-of-three, since each image channel corresponds to red, green, and blue pixel values. \n",
    ">\n",
    "> As always, refer to previous notebook documentation on image normalization for colorization if you need help.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing_layer = Resizing(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "normalization_layer = Rescaling(1./255, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Architecture Creation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Now time for the main event! \n",
    "> \n",
    "> Here, you'll be creating and instantiating your model architecture. \n",
    "> \n",
    "> For this assignment, you'll be creating a **Convolutional Neural Network** that can process Fire Detection images for predictive purposes.\n",
    "> \n",
    "> _For this project, you will not be provided guidance as to how to design and implement your CNN architecture._\n",
    "> \n",
    "> Refer to previous notebooks and challenges on CNNs as well as online documentation/resources for how to design CNN models on higher-order images. \n",
    "> \n",
    "> **This is a highly creative step, and there are no wrong answers; however, you will be assessed on your experimentation process and why you choose specific modeling layers, configurations, optimizers, regularizers, and overall design choices.**\n",
    ">\n",
    "> Light boilerplate will be provided to get you started, but as always, use any and all resources at your disposal to finish the job! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "random_flipping_layer = RandomFlip(\"horizontal\", input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "random_rotation_layer = RandomRotation(0.1)\n",
    "random_zooming_layer = RandomZoom(0.2)\n",
    "\n",
    "augmentation_layer = Sequential([random_flipping_layer, \n",
    "                                       random_rotation_layer, \n",
    "                                       random_zooming_layer])\n",
    "\n",
    "rescaling_layer = Rescaling(scale=1/127.5, offset=-1)\n",
    "\n",
    "first_convolutional_layer = Conv2D(16, \n",
    "                                   kernel_size=(3, 3), \n",
    "                                   activation=\"relu\")\n",
    "second_convolutional_layer = Conv2D(32,\n",
    "                                    kernel_size=(3, 3),\n",
    "                                    activation=\"relu\")\n",
    "third_convolutional_layer = Conv2D(64,\n",
    "                                   kernel_size=(3, 3),\n",
    "                                   activation=\"relu\")\n",
    "fourth_convolutional_layer = Conv2D(128,\n",
    "                                    kernel_size=(3, 3),\n",
    "                                    activation=\"relu\")\n",
    "\n",
    "first_pooling_layer = MaxPool2D(pool_size=(2, 2))\n",
    "second_pooling_layer = MaxPool2D(pool_size=(2, 2))\n",
    "third_pooling_layer = MaxPool2D(pool_size=(2, 2))\n",
    "fourth_pooling_layer = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "flattening_layer = Flatten()\n",
    "\n",
    "first_connective_layer = Dense(512, activation=\"relu\")\n",
    "second_connective_layer = Dense(512, activation=\"relu\")\n",
    "output_connective_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "first_dropout_layer = Dropout(0.3)\n",
    "second_dropout_layer = Dropout(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_4 (Sequential)    (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "rescaling_6 (Rescaling)      (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 254, 254, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPoolin  (None, 127, 127, 16)      0         \n",
      "g2D)                                                             \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPoolin  (None, 62, 62, 32)        0         \n",
      "g2D)                                                             \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      "g2D)                                                             \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      "g2D)                                                             \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Total params: 13,206,177\n",
      "Trainable params: 13,206,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sequential Model Architecture Setup \"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\" CNN Layering Steps \"\"\"\n",
    "# TODO: Instantiate and Add Layers to Create a Functional CNN for Image Recognition.\n",
    "# TODO: Be Sure to Use as Many Imported Layers at the Top of the Notebook as Possible. \n",
    "model.add(input_layer)\n",
    "model.add(augmentation_layer)\n",
    "model.add(rescaling_layer)\n",
    "model.add(first_convolutional_layer)\n",
    "model.add(first_pooling_layer)\n",
    "model.add(second_convolutional_layer)\n",
    "model.add(second_pooling_layer)\n",
    "model.add(third_convolutional_layer)\n",
    "model.add(third_pooling_layer)\n",
    "model.add(fourth_convolutional_layer)\n",
    "model.add(fourth_pooling_layer)\n",
    "model.add(flattening_layer)\n",
    "model.add(first_connective_layer)\n",
    "model.add(first_dropout_layer)\n",
    "model.add(second_connective_layer)\n",
    "model.add(second_dropout_layer)\n",
    "model.add(output_connective_layer)\n",
    "\n",
    "\"\"\" CNN Architecture Summarization \"\"\"\n",
    "# TODO: Save Screenshots of your Model Summaries to put in this Project Subfolder.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Configuration ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll compile your CNN architecture with appropriate parameters for loss calculation, optimization, and accuracy metrics.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for best-case parameters to use for image recognition models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CNN Model Compilation \"\"\"\n",
    "# TODO: Compile Model with Appropriate Loss, Optimizer, and Metrics-Based Parameters\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=\"Adamax\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Predictive Fitness ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll be taking your compiled model and fitting it against your training and validation data.\n",
    "> \n",
    "> Keep in mind that there are several opportunities for further optimizing your workflow with techniques such as batch normalization, generator-based data feeding, etc. \n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for designing model fitness with validation data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "28/28 [==============================] - 29s 996ms/step - loss: 0.4511 - accuracy: 0.7737 - val_loss: 0.3291 - val_accuracy: 0.8333\n",
      "Epoch 2/15\n",
      "28/28 [==============================] - 24s 861ms/step - loss: 0.3190 - accuracy: 0.8788 - val_loss: 0.2050 - val_accuracy: 0.9352\n",
      "Epoch 3/15\n",
      "28/28 [==============================] - 28s 986ms/step - loss: 0.2398 - accuracy: 0.9099 - val_loss: 0.2049 - val_accuracy: 0.9213\n",
      "Epoch 4/15\n",
      "28/28 [==============================] - 27s 954ms/step - loss: 0.2120 - accuracy: 0.9157 - val_loss: 0.1610 - val_accuracy: 0.9444\n",
      "Epoch 5/15\n",
      "28/28 [==============================] - 26s 947ms/step - loss: 0.1676 - accuracy: 0.9307 - val_loss: 0.1929 - val_accuracy: 0.9352\n",
      "Epoch 6/15\n",
      "28/28 [==============================] - 26s 932ms/step - loss: 0.1419 - accuracy: 0.9503 - val_loss: 0.1906 - val_accuracy: 0.9537\n",
      "Epoch 7/15\n",
      "28/28 [==============================] - 27s 960ms/step - loss: 0.1330 - accuracy: 0.9550 - val_loss: 0.1308 - val_accuracy: 0.9491\n",
      "Epoch 8/15\n",
      "28/28 [==============================] - 27s 961ms/step - loss: 0.1184 - accuracy: 0.9561 - val_loss: 0.1869 - val_accuracy: 0.9630\n",
      "Epoch 9/15\n",
      "28/28 [==============================] - 27s 960ms/step - loss: 0.1145 - accuracy: 0.9596 - val_loss: 0.1483 - val_accuracy: 0.9398\n",
      "Epoch 10/15\n",
      "28/28 [==============================] - 27s 978ms/step - loss: 0.1136 - accuracy: 0.9654 - val_loss: 0.0998 - val_accuracy: 0.9676\n",
      "Epoch 11/15\n",
      "28/28 [==============================] - 27s 967ms/step - loss: 0.0719 - accuracy: 0.9746 - val_loss: 0.1300 - val_accuracy: 0.9630\n",
      "Epoch 12/15\n",
      "28/28 [==============================] - 27s 952ms/step - loss: 0.0805 - accuracy: 0.9746 - val_loss: 0.1321 - val_accuracy: 0.9676\n",
      "Epoch 13/15\n",
      "28/28 [==============================] - 26s 942ms/step - loss: 0.0843 - accuracy: 0.9723 - val_loss: 0.1159 - val_accuracy: 0.9583\n",
      "Epoch 14/15\n",
      "28/28 [==============================] - 27s 959ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.0981 - val_accuracy: 0.9630\n",
      "Epoch 15/15\n",
      "28/28 [==============================] - 27s 974ms/step - loss: 0.0679 - accuracy: 0.9815 - val_loss: 0.1139 - val_accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "\"\"\" CNN Model Fitness and History Extraction \"\"\"\n",
    "# TODO: Fit Model Against Training and Validation Data with Appropriate Epochs\n",
    "epochs = 15\n",
    "history = model.fit(train,\n",
    "                    validation_data=validation,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Evaluation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll evaluate your CNN model using the validation dataset in order to calculate overall validation accuracy and loss.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for using the proper evaluation function for model prediction. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 226ms/step - loss: 0.1139 - accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11386638134717941, 0.9583333134651184]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" CNN Model Predictive Evaluation \"\"\"\n",
    "# TODO: Evaluate Model Against Validation Dataset\n",
    "model.evaluate(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Model State Saving\n",
    "\n",
    "When you are satisfied with your model state configuration and performance and are ready to export the model's weights and parameters for deployment purposes, simply run the following function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name, save_format):\n",
    "    \"\"\" \n",
    "    Save the model weights and architecture.\n",
    "    \n",
    "    Parameters: \n",
    "       model(Model): keras Model object being saved\n",
    "       file_name(str): name of the Hadoop file where\n",
    "                       the whole model will be saved\n",
    "       save_format(str): Indicates whether to save the model to the default\n",
    "                         SavedModel('tf'), or HDF5('h5'), or \n",
    "                         use both H5 and JSON ('composite') formats. \n",
    "       Returns: None\n",
    "    \"\"\"\n",
    "    MODEL_DIRECTORY = \"../model\"\n",
    "    def __save_as_composite():\n",
    "      \"\"\" Saving the model as H5 (for params) + JSON (for the architecture) \"\"\"\n",
    "      # Save the weights\n",
    "      model.save_weights(f'{MODEL_DIRECTORY}/{file_name}_params.h5')\n",
    "      # Save the architecture\n",
    "      with open(f'{MODEL_DIRECTORY}/{file_name}_layers.json', 'w') as f:\n",
    "          f.write(model.to_json())\n",
    "    \n",
    "    def __save_as_h5():\n",
    "      \"\"\" Option 2: Saving whole model as a single H5 file (more storage) \"\"\"\n",
    "      model.save(f\"{MODEL_DIRECTORY}/{file_name}.h5\", save_format=save_format)\n",
    "\n",
    "    # Call the appropiate save func\n",
    "    if save_format == 'h5':\n",
    "      __save_as_h5()\n",
    "    elif save_format == 'composite':\n",
    "      __save_as_composite()\n",
    "    else:  # save as a SavedModel\n",
    "      model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"fire_cnn_classifier\", \"composite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and complete the remaining tasks in `project/PROJECT.md` to complete this project successfully! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
